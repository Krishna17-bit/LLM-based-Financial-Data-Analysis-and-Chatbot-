{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fitz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtq-DGNNvUp4",
        "outputId": "a1a7fa26-0069-493c-b75c-b097da7281bf",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fitz\n",
            "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl.metadata (816 bytes)\n",
            "Collecting configobj (from fitz)\n",
            "  Downloading configobj-5.0.8-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting configparser (from fitz)\n",
            "  Downloading configparser-7.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.10/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from fitz) (5.0.1)\n",
            "Collecting nipype (from fitz)\n",
            "  Downloading nipype-1.8.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fitz) (2.1.4)\n",
            "Collecting pyxnat (from fitz)\n",
            "  Downloading pyxnat-1.6.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from configobj->fitz) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2->fitz) (3.1.4)\n",
            "Requirement already satisfied: packaging>=17 in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (71.0.4)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (8.1.7)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.3)\n",
            "Collecting prov>=1.5.2 (from nipype->fitz)\n",
            "  Downloading prov-2.0.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.8.2)\n",
            "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting simplejson>=3.8.0 (from nipype->fitz)\n",
            "  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting traits!=5.0,<6.4,>=4.6 (from nipype->fitz)\n",
            "  Downloading traits-6.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.16.0)\n",
            "Collecting etelemetry>=0.2.0 (from nipype->fitz)\n",
            "  Downloading etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting looseversion (from nipype->fitz)\n",
            "  Downloading looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.1)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (4.9.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (2.32.3)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Collecting ci-info>=0.2 (from etelemetry>=0.2.0->nipype->fitz)\n",
            "  Downloading ci_info-0.3.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
            "  Downloading rdflib-6.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=5.0.0->nipype->fitz)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (2024.8.30)\n",
            "Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
            "Downloading configparser-7.1.0-py3-none-any.whl (17 kB)\n",
            "Downloading nipype-1.8.6-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyxnat-1.6.2-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
            "Downloading prov-2.0.1-py3-none-any.whl (421 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traits-6.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: looseversion, traits, simplejson, isodate, configparser, configobj, ci-info, rdflib, pyxnat, etelemetry, prov, nipype, fitz\n",
            "Successfully installed ci-info-0.3.0 configobj-5.0.8 configparser-7.1.0 etelemetry-0.3.1 fitz-0.0.1.dev2 isodate-0.6.1 looseversion-1.3.0 nipype-1.8.6 prov-2.0.1 pyxnat-1.6.2 rdflib-6.3.2 simplejson-3.19.3 traits-6.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install frontend"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_0xirIbvfzK",
        "outputId": "7643c50d-3159-4565-b372-40f82d099104",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting frontend\n",
            "  Downloading frontend-0.0.3-py3-none-any.whl.metadata (847 bytes)\n",
            "Collecting starlette>=0.12.0 (from frontend)\n",
            "  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting uvicorn>=0.7.1 (from frontend)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from frontend) (2.2.0)\n",
            "Collecting aiofiles (from frontend)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.12.0->frontend) (3.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.7.1->frontend)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.7.1->frontend) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.12.0->frontend) (1.2.2)\n",
            "Downloading frontend-0.0.3-py3-none-any.whl (32 kB)\n",
            "Downloading starlette-0.38.5-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, aiofiles, uvicorn, starlette, frontend\n",
            "Successfully installed aiofiles-24.1.0 frontend-0.0.3 h11-0.14.0 starlette-0.38.5 uvicorn-0.30.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5noKZXaUvyJs",
        "outputId": "1c96f69c-472d-4f56-9d27-06eb1ccec7b2",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.10-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.24.10 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.10-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.10-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.10-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.10 PyMuPDFb-1.24.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF for reading PDFs\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(file_path):\n",
        "    document = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in document:\n",
        "        text += page.get_text()\n",
        "    document.close()\n",
        "    return text\n",
        "\n",
        "# Paths to your PDF files\n",
        "pdf_paths = [\n",
        "    '/content/Infosys_2021.pdf',\n",
        "    '/content/Infosys_2022.pdf',\n",
        "    '/content/Infosys_2023.pdf',\n",
        "    '/content/sa-fy23-annual-finstatement.pdf'\n",
        "]\n",
        "\n",
        "# Extract and print raw text from each PDF\n",
        "for i, pdf_path in enumerate(pdf_paths):\n",
        "    print(f\"Raw text from PDF {i+1} ({pdf_path}):\\n\")\n",
        "    raw_text = extract_text_from_pdf(pdf_path)\n",
        "    print(raw_text[:2000])  # Display the first 2000 characters of the text\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")  # Separator between PDFs\n"
      ],
      "metadata": {
        "id": "jenO_nhHfCaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    \"\"\"Extract text from each page of the given PDF.\"\"\"\n",
        "    document = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in document:\n",
        "        text += page.get_text()\n",
        "    document.close()\n",
        "    return text\n",
        "\n",
        "def parse_financial_data(text):\n",
        "    \"\"\"Parse financial data from extracted text to create a structured DataFrame.\"\"\"\n",
        "    lines = text.splitlines()\n",
        "    data = []\n",
        "    description = \"\"\n",
        "    note_no = \"\"\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        # Regex to find numeric patterns that may include commas\n",
        "        amounts = re.findall(r'\\d{1,3}(?:,\\d{3})*', line)\n",
        "\n",
        "        if amounts:\n",
        "            if len(amounts) >= 2:  # Ensure there are at least two amounts to consider them as 2020 and 2021 data\n",
        "                # Only consider the first two amounts\n",
        "                amount_2020, amount_2021 = amounts[:2]\n",
        "                if description:  # Ensure there is a description to pair with the amounts\n",
        "                    data.append([note_no, description.strip(), amount_2020, amount_2021])\n",
        "                description = \"\"  # Reset description for the next entry\n",
        "        else:\n",
        "            note_match = re.match(r'(\\d+\\.\\d+)', line)\n",
        "            if note_match:\n",
        "                if description and note_no:\n",
        "                    data.append([note_no, description.strip(), \"\", \"\"])  # Append with empty amounts if no numeric data followed\n",
        "                note_no = note_match.group(1)\n",
        "                description = line.replace(note_no, '').strip()\n",
        "            else:\n",
        "                description += \" \" + line  # Continue building the description\n",
        "\n",
        "    # Create DataFrame with specific column names for the years\n",
        "    df = pd.DataFrame(data, columns=[\"Note No.\", \"Description\", \"Amount 2020\", \"Amount 2021\"])\n",
        "    return df\n",
        "\n",
        "# Specify the PDF file\n",
        "pdf_path = '/content/Consolidated and Standalone.pdf'  # Adjust the path to your specific file\n",
        "\n",
        "# Extract text and parse financial data\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "df = parse_financial_data(text)\n",
        "\n",
        "# Display the parsed data\n",
        "print(df.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0CiqiKZDDjx",
        "outputId": "13c319a6-da9c-4316-f424-39c66289ca5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Note No.                                        Description Amount 2020  \\\n",
            "0                                                   Year ended         202   \n",
            "1                                      Revenue from operations      45,411   \n",
            "2                                                 Other income         721   \n",
            "3                                                 TOTAL INCOME      46,132   \n",
            "4                           Expenses Employee benefit expenses      25,649   \n",
            "5                                 Fees to external consultants       3,900   \n",
            "6                      Cost of equipment and software licences         258   \n",
            "7                        Depreciation and amortisation expense       1,075   \n",
            "8                                               Other expenses       2,941   \n",
            "9                                               TOTAL EXPENSES      33,823   \n",
            "10           PROFIT BEFORE FINANCE COSTS, EXCEPTIONAL ITEM ...      12,309   \n",
            "11                                               Finance costs         146   \n",
            "12                      PROFIT BEFORE EXCEPTIONAL ITEM AND TAX      12,163   \n",
            "13             Provision towards legal claim PROFIT BEFORE TAX      12,163   \n",
            "14                                     Tax expense Current tax       3,138   \n",
            "15                                                Deferred tax           6   \n",
            "16                                           TOTAL TAX EXPENSE       3,132   \n",
            "17                                       PROFIT FOR THE PERIOD       9,031   \n",
            "18           Profit for the period attributable to: Shareho...       9,008   \n",
            "19                                   Non-controlling interests          23   \n",
            "\n",
            "   Amount 2021  \n",
            "0            1  \n",
            "1       43,705  \n",
            "2          931  \n",
            "3       44,636  \n",
            "4       23,625  \n",
            "5        3,874  \n",
            "6          454  \n",
            "7        1,067  \n",
            "8        2,951  \n",
            "9       31,971  \n",
            "10      12,665  \n",
            "11         138  \n",
            "12      12,527  \n",
            "13      12,527  \n",
            "14       2,879  \n",
            "15         366  \n",
            "16       3,245  \n",
            "17       9,282  \n",
            "18       9,246  \n",
            "19          36  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to a CSV file\n",
        "csv_path = '/content/parsed_financial_data.csv'\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(f\"Data saved to {csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFOzS-_iEXXu",
        "outputId": "1f93b14f-0117-4555-9f9e-a5a44fdbe8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to /content/parsed_financial_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "infosys_files = [\n",
        "    '/content/infosys 2021.csv',\n",
        "    '/content/infosys 2022.csv',\n",
        "    '/content/infosys 2023.csv',\n",
        "    '/content/infosys fy 2023.csv'\n",
        "]\n",
        "\n",
        "tcs_files = [\n",
        "    '/content/TCS 2020.csv',\n",
        "    '/content/TCS 2022.csv',\n",
        "    '/content/TCS 2020-2021.csv'\n",
        "]\n",
        "\n",
        "# Function to load and display column headers from each file\n",
        "def display_headers(file_list, company_name):\n",
        "    print(f\"Headers for {company_name} files:\")\n",
        "    for file_path in file_list:\n",
        "        try:\n",
        "            # Read only the first row to get the headers\n",
        "            df = pd.read_csv(file_path, nrows=0)  # nrows=0 loads no data rows, just headers\n",
        "            print(f\"\\nHeaders from {file_path.split('/')[-1]}:\")\n",
        "            print(df.columns.tolist())  # Display the list of column names\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to read headers from {file_path}: {e}\")\n",
        "\n",
        "# Display headers for Infosys and TCS files\n",
        "display_headers(infosys_files, \"Infosys\")\n",
        "display_headers(tcs_files, \"TCS\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTec1UMeKOpH",
        "outputId": "2def732b-70e8-401d-82ca-d35f4eb227dd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Headers for Infosys files:\n",
            "\n",
            "Headers from infosys 2021.csv:\n",
            "['Note No.', 'Description', 'Amount 2020', 'Amount 2021']\n",
            "\n",
            "Headers from infosys 2022.csv:\n",
            "['Note No.', 'Description', 'Amount 2021', 'Amount 2022']\n",
            "\n",
            "Headers from infosys 2023.csv:\n",
            "['Note No.', 'Description', 'Amount 2022', 'Amount 2023']\n",
            "\n",
            "Headers from infosys fy 2023.csv:\n",
            "['Note No.', 'Description', 'Amount 2022', 'Amount 2023']\n",
            "Headers for TCS files:\n",
            "\n",
            "Headers from TCS 2020.csv:\n",
            "['Note No.', 'Description', 'Amount 2019', 'Amount 2020']\n",
            "\n",
            "Headers from TCS 2022.csv:\n",
            "['Note No.', 'Description', 'Amount 2021', 'Amount 2022']\n",
            "\n",
            "Headers from TCS 2020-2021.csv:\n",
            "['Note No.', 'Description', 'Amount 2020', 'Amount 2021']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install libraries for setting up for tarning using LLM\n",
        "# Install transformers and any other necessary libraries\n",
        "!pip install transformers torch --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbCTAkNMwHJD",
        "outputId": "741c9e31-d557-456c-899f-8b1c7b50b7ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Collecting torch\n",
            "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.22.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.22.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.22.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.0+cu121\n",
            "    Uninstalling torch-2.4.0+cu121:\n",
            "      Successfully uninstalled torch-2.4.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.4.0+cu121 requires torch==2.4.0, but you have torch 2.4.1 which is incompatible.\n",
            "torchvision 0.19.0+cu121 requires torch==2.4.0, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 triton-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "BFC1Cwkzwgw5"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import fitz\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ],
      "metadata": {
        "id": "4o0oMWwsKQJy"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load a Pre-trained Model and Tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCHG24bPwgum",
        "outputId": "2c7cc548-7e4f-4e39-fa38-d63cd2e4ec79"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Dictionary to hold DataFrames keyed by year/company\n",
        "dataframes = {\n",
        "    'Infosys 2021': pd.read_csv('/content/infosys 2021.csv'),\n",
        "    'Infosys 2022': pd.read_csv('/content/infosys 2022.csv'),\n",
        "    'Infosys 2023': pd.read_csv('/content/infosys 2023.csv'),\n",
        "    'Infosys FY 2023': pd.read_csv('/content/infosys fy 2023.csv'),\n",
        "    'TCS 2020': pd.read_csv('/content/TCS 2020.csv'),\n",
        "    'TCS 2022': pd.read_csv('/content/TCS 2022.csv'),\n",
        "    'TCS 2020-2021': pd.read_csv('/content/TCS 2020-2021.csv')\n",
        "}\n",
        "\n",
        "# Example usage: print the head of the Infosys 2021 data\n",
        "print(dataframes['Infosys 2022'].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49wMDYqDOrqy",
        "outputId": "85fe2975-3b3d-46d7-e344-b72d5fbddd5e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Note No.                            Description Amount 2021 Amount 2022\n",
            "0       NaN  INFOSYS LIMITED (In ₹ crore) Note No.          31         202\n",
            "1       NaN          Property, plant and equipment           2           1\n",
            "2       NaN                    Right-of-use assets           2           3\n",
            "3       NaN               Capital work-in-progress         411         906\n",
            "4       NaN                               Goodwill           2           2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Dictionary setup\n",
        "dataframes = {\n",
        "    'Infosys 2021': pd.read_csv('/content/infosys 2021.csv'),\n",
        "    'Infosys 2022': pd.read_csv('/content/infosys 2022.csv'),\n",
        "    'Infosys 2023': pd.read_csv('/content/infosys 2023.csv'),\n",
        "    'Infosys FY 2023': pd.read_csv('/content/infosys fy 2023.csv'),\n",
        "    'TCS 2020': pd.read_csv('/content/TCS 2020.csv'),\n",
        "    'TCS 2022': pd.read_csv('/content/TCS 2022.csv'),\n",
        "    'TCS 2020-2021': pd.read_csv('/content/TCS 2020-2021.csv')\n",
        "}\n",
        "\n",
        "# Initialize a dictionary to hold the labels for each year\n",
        "combined_labels = {}\n",
        "\n",
        "# Preprocess data and generate labels for each year present in the DataFrames\n",
        "for key, df in dataframes.items():\n",
        "    # Detect amount columns present in the current DataFrame\n",
        "    amount_columns = [col for col in df.columns if 'Amount' in col]\n",
        "    # Preprocess each amount column found\n",
        "    for column in amount_columns:\n",
        "        # Remove commas and convert to numeric\n",
        "        df[column] = pd.to_numeric(df[column].replace(',', '', regex=True), errors='coerce')\n",
        "        # Create labels based on a threshold (e.g., 1000)\n",
        "        labels = df[column].apply(lambda x: 1 if x > 1000 else 0).tolist()\n",
        "        # Store the labels with a key indicating the DataFrame and year\n",
        "        combined_labels[f'labels_{column}_{key}'] = labels\n",
        "\n",
        "    # Update the dictionary with the modified DataFrame\n",
        "    dataframes[key] = df\n",
        "\n",
        "#  printing the modified DataFrame and labels\n",
        "print(dataframes['Infosys 2021'].head())\n",
        "print(combined_labels.get('labels_Amount 2021_Infosys 2021'))\n",
        "print(dataframes['Infosys 2022'].head())\n",
        "print(combined_labels.get('labels_Amount 2022_Infosys 2022'))\n",
        "print(dataframes['Infosys 2023'].head())\n",
        "print(combined_labels.get('labels_Amount 2023_Infosys 2023'))\n",
        "print(dataframes['Infosys FY 2023'].head())\n",
        "print(combined_labels.get('labels_Amount 2023_Infosys FY  2023'))\n",
        "print(dataframes['TCS 2020'].head())\n",
        "print(combined_labels.get('labels_Amount 2020_TCS 2020'))\n",
        "print(dataframes['TCS 2022'].head())\n",
        "print(combined_labels.get('labels_Amount 2022_TCS 2022'))\n",
        "print(dataframes['TCS 2020-2021'].head())\n",
        "print(combined_labels.get('labels_Amount 2020_TCS 2020-2021'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RYOIkoaAINvp",
        "outputId": "2607008a-b4d9-4d31-a9f2-a3087f4e4cb0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Note No.                            Description  Amount 2020  Amount 2021\n",
            "0       NaN  INFOSYS LIMITED (In ₹ crore) Note No.           31          202\n",
            "1       NaN          Property, plant and equipment            2            1\n",
            "2       NaN                    Right-of-use assets            2            2\n",
            "3       NaN               Capital work-in-progress          906          945\n",
            "4       NaN                               Goodwill          167           29\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "   Note No.                            Description  Amount 2021  Amount 2022\n",
            "0       NaN  INFOSYS LIMITED (In ₹ crore) Note No.           31          202\n",
            "1       NaN          Property, plant and equipment            2            1\n",
            "2       NaN                    Right-of-use assets            2            3\n",
            "3       NaN               Capital work-in-progress          411          906\n",
            "4       NaN                               Goodwill            2            2\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "   Note No.                            Description  Amount 2022  Amount 2023\n",
            "0       NaN  INFOSYS LIMITED (In ₹ crore) Note No.           31          202\n",
            "1       NaN          Property, plant and equipment            2            1\n",
            "2       NaN                    Right-of-use assets            2            3\n",
            "3       NaN               Capital work-in-progress            2            4\n",
            "4       NaN                               Goodwill            2            2\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1]\n",
            "   Note No.                            Description  Amount 2022  Amount 2023\n",
            "0       NaN  INFOSYS LIMITED (In ₹ crore) Note No.           31          202\n",
            "1       NaN          Property, plant and equipment            2            1\n",
            "2       NaN                    Right-of-use assets            2            3\n",
            "3       NaN               Capital work-in-progress            2            4\n",
            "4       NaN                               Goodwill            2            2\n",
            "None\n",
            "   Note No.                                        Description  Amount 2019  \\\n",
            "0       NaN                               (` crore) Note As at           30   \n",
            "1       NaN                                              As at           31   \n",
            "2       NaN  ASSETS Non-current assets Property, plant and ...            1   \n",
            "3       NaN  EQUITY AND LIABILITIES Equity Share capital Ot...         6923   \n",
            "4       NaN  Other financial liabilities Unearned and defer...          641   \n",
            "\n",
            "   Amount 2020  \n",
            "0          202  \n",
            "1          202  \n",
            "2        26661  \n",
            "3         6906  \n",
            "4          697  \n",
            "[0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "   Note No.                                        Description  Amount 2021  \\\n",
            "0       NaN                               (` crore) Note As at           30   \n",
            "1       NaN                                              As at           31   \n",
            "2       NaN  ASSETS Non-current assets Property, plant and ...            1   \n",
            "3       NaN                                       TOTAL ASSETS            1   \n",
            "4       NaN  EQUITY AND LIABILITIES Equity Share capital Ot...            1   \n",
            "\n",
            "   Amount 2022  \n",
            "0          202  \n",
            "1          202  \n",
            "2         7964  \n",
            "3        40552  \n",
            "4        40552  \n",
            "[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "   Note No.                         Description  Amount 2020  Amount 2021\n",
            "0       NaN                          Year ended          202            1\n",
            "1       NaN             Revenue from operations        45411        43705\n",
            "2       NaN                        Other income          721          931\n",
            "3       NaN                        TOTAL INCOME        46132        44636\n",
            "4       NaN  Expenses Employee benefit expenses        25649        23625\n",
            "[0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataframes = {\n",
        "    'Infosys 2021': pd.read_csv('/content/infosys 2021.csv'),\n",
        "    'Infosys 2022': pd.read_csv('/content/infosys 2022.csv'),\n",
        "    'Infosys 2023': pd.read_csv('/content/infosys 2023.csv'),\n",
        "    'Infosys FY 2023': pd.read_csv('/content/infosys fy 2023.csv'),\n",
        "    'TCS 2020': pd.read_csv('/content/TCS 2020.csv'),\n",
        "    'TCS 2022': pd.read_csv('/content/TCS 2022.csv'),\n",
        "    'TCS 2020-2021': pd.read_csv('/content/TCS 2020-2021.csv')\n",
        "}\n",
        "\n",
        "# Iterate over each DataFrame to print details for any columns that include 'Amount'\n",
        "for key, df in dataframes.items():\n",
        "    # Identify amount columns present in the DataFrame\n",
        "    amount_columns = [col for col in df.columns if 'Amount' in col]\n",
        "    # Continue only if there are amount columns in the DataFrame\n",
        "    if amount_columns:\n",
        "        # Check the data types of the Amount columns\n",
        "        print(f\"Data Types for {key}:\")\n",
        "        print(df[amount_columns].dtypes)\n",
        "\n",
        "        # Display the first few rows of the Amount columns\n",
        "        print(f\"\\nFirst Few Rows for {key}:\")\n",
        "        print(df[amount_columns].head())\n",
        "\n",
        "        # Display descriptive statistics for the Amount columns\n",
        "        print(f\"\\nDescriptive Statistics for {key}:\")\n",
        "        print(df[amount_columns].describe())\n",
        "\n",
        "        # Add a separator for readability between different data outputs\n",
        "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdBy0HyKIpgg",
        "outputId": "773b6227-aedf-47f8-bea2-bba32031b5fb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Types for Infosys 2021:\n",
            "Amount 2020    object\n",
            "Amount 2021    object\n",
            "dtype: object\n",
            "\n",
            "First Few Rows for Infosys 2021:\n",
            "  Amount 2020 Amount 2021\n",
            "0          31         202\n",
            "1           2           1\n",
            "2           2           2\n",
            "3         906         945\n",
            "4         167          29\n",
            "\n",
            "Descriptive Statistics for Infosys 2021:\n",
            "       Amount 2020 Amount 2021\n",
            "count           95          95\n",
            "unique          42          54\n",
            "top              2         202\n",
            "freq            39           7\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Data Types for Infosys 2022:\n",
            "Amount 2021    object\n",
            "Amount 2022    object\n",
            "dtype: object\n",
            "\n",
            "First Few Rows for Infosys 2022:\n",
            "  Amount 2021 Amount 2022\n",
            "0          31         202\n",
            "1           2           1\n",
            "2           2           3\n",
            "3         411         906\n",
            "4           2           2\n",
            "\n",
            "Descriptive Statistics for Infosys 2022:\n",
            "       Amount 2021 Amount 2022\n",
            "count           96          96\n",
            "unique          40          55\n",
            "top              2         202\n",
            "freq            42           8\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Data Types for Infosys 2023:\n",
            "Amount 2022    object\n",
            "Amount 2023    object\n",
            "dtype: object\n",
            "\n",
            "First Few Rows for Infosys 2023:\n",
            "  Amount 2022 Amount 2023\n",
            "0          31         202\n",
            "1           2           1\n",
            "2           2           3\n",
            "3           2           4\n",
            "4           2           2\n",
            "\n",
            "Descriptive Statistics for Infosys 2023:\n",
            "       Amount 2022 Amount 2023\n",
            "count          132         132\n",
            "unique          56          67\n",
            "top              2           5\n",
            "freq            59          11\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Data Types for Infosys FY 2023:\n",
            "Amount 2022    object\n",
            "Amount 2023    object\n",
            "dtype: object\n",
            "\n",
            "First Few Rows for Infosys FY 2023:\n",
            "  Amount 2022 Amount 2023\n",
            "0          31         202\n",
            "1           2           1\n",
            "2           2           3\n",
            "3           2           4\n",
            "4           2           2\n",
            "\n",
            "Descriptive Statistics for Infosys FY 2023:\n",
            "       Amount 2022 Amount 2023\n",
            "count          132         132\n",
            "unique          56          67\n",
            "top              2           5\n",
            "freq            59          11\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Data Types for TCS 2020:\n",
            "Amount 2019    object\n",
            "Amount 2020    object\n",
            "dtype: object\n",
            "\n",
            "First Few Rows for TCS 2020:\n",
            "  Amount 2019 Amount 2020\n",
            "0          30         202\n",
            "1          31         202\n",
            "2           1      26,661\n",
            "3       6,923       6,906\n",
            "4         641         697\n",
            "\n",
            "Descriptive Statistics for TCS 2020:\n",
            "       Amount 2019 Amount 2020\n",
            "count          325         325\n",
            "unique         146         159\n",
            "top             30         202\n",
            "freq            65          86\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Data Types for TCS 2022:\n",
            "Amount 2021    object\n",
            "Amount 2022    object\n",
            "dtype: object\n",
            "\n",
            "First Few Rows for TCS 2022:\n",
            "  Amount 2021 Amount 2022\n",
            "0          30         202\n",
            "1          31         202\n",
            "2           1      07,964\n",
            "3           1      40,552\n",
            "4           1      40,552\n",
            "\n",
            "Descriptive Statistics for TCS 2022:\n",
            "       Amount 2021 Amount 2022\n",
            "count          347         347\n",
            "unique         162         167\n",
            "top             30         202\n",
            "freq            66         110\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Data Types for TCS 2020-2021:\n",
            "Amount 2020    object\n",
            "Amount 2021    object\n",
            "dtype: object\n",
            "\n",
            "First Few Rows for TCS 2020-2021:\n",
            "  Amount 2020 Amount 2021\n",
            "0         202           1\n",
            "1      45,411      43,705\n",
            "2         721         931\n",
            "3      46,132      44,636\n",
            "4      25,649      23,625\n",
            "\n",
            "Descriptive Statistics for TCS 2020-2021:\n",
            "       Amount 2020 Amount 2021\n",
            "count           99          99\n",
            "unique          75          72\n",
            "top              7         202\n",
            "freq             4           8\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Dictionary setup\n",
        "dataframes = {\n",
        "    'Infosys 2021': pd.read_csv('/content/infosys 2021.csv'),\n",
        "    'Infosys 2022': pd.read_csv('/content/infosys 2022.csv'),\n",
        "    'Infosys 2023': pd.read_csv('/content/infosys 2023.csv'),\n",
        "    'Infosys FY 2023': pd.read_csv('/content/infosys fy 2023.csv'),\n",
        "    'TCS 2020': pd.read_csv('/content/TCS 2020.csv'),\n",
        "    'TCS 2022': pd.read_csv('/content/TCS 2022.csv'),\n",
        "    'TCS 2020-2021': pd.read_csv('/content/TCS 2020-2021.csv')\n",
        "}\n",
        "\n",
        "# Initialize lists to collect all descriptions and amounts\n",
        "descriptions = []\n",
        "amounts_dict = {}\n",
        "\n",
        "# Iterate through each DataFrame\n",
        "for key, df in dataframes.items():\n",
        "    # Handle NaN values: Replace NaN with 0 for relevant Amount columns\n",
        "    amount_columns = [col for col in df.columns if 'Amount' in col]\n",
        "    df[amount_columns] = df[amount_columns].fillna(0)\n",
        "\n",
        "    # Append descriptions\n",
        "    descriptions.extend(df['Description'].tolist())\n",
        "\n",
        "    # Collect amounts from each column\n",
        "    for column in amount_columns:\n",
        "        if column not in amounts_dict:\n",
        "            amounts_dict[column] = []\n",
        "        amounts_dict[column].extend(df[column].tolist())\n",
        "\n",
        "# Ensure all lists are of the same length\n",
        "max_length = len(descriptions)\n",
        "for amount_col, amounts in amounts_dict.items():\n",
        "    if len(amounts) != max_length:\n",
        "        # Pad the amounts list with zeros if they are not of the same length\n",
        "        amounts.extend([0] * (max_length - len(amounts)))\n",
        "\n",
        "# Create the DataFrame\n",
        "response_df = pd.DataFrame({\n",
        "    'Description': descriptions\n",
        "})\n",
        "for amount_col, amounts in amounts_dict.items():\n",
        "    response_df[amount_col] = amounts\n",
        "\n",
        "# Print the first 20 entries to verify\n",
        "print(response_df.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxDylBBAJjL4",
        "outputId": "8389afab-f318-49f2-d97c-50857568529c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    Description Amount 2020 Amount 2021  \\\n",
            "0         INFOSYS LIMITED (In ₹ crore) Note No.          31         202   \n",
            "1                 Property, plant and equipment           2           1   \n",
            "2                           Right-of-use assets           2           2   \n",
            "3                      Capital work-in-progress         906         945   \n",
            "4                                      Goodwill         167          29   \n",
            "5                       Other intangible assets          67          48   \n",
            "6                  Financial assets Investments           2           3   \n",
            "7                                         Loans           2           4   \n",
            "8                        Other financial assets           2           5   \n",
            "9                     Deferred tax assets (net)         955       1,429   \n",
            "10                      Income tax assets (net)       5,287       4,773   \n",
            "11                     Other non-current assets           2           8   \n",
            "12  Current assets Financial assets Investments           2           3   \n",
            "13                            Trade receivables           2           6   \n",
            "14                    Cash and cash equivalents           2           7   \n",
            "15                                        Loans           2           4   \n",
            "16                       Other financial assets           2           5   \n",
            "17                         Other current assets           2           8   \n",
            "18                  Equity Equity share capital           2          10   \n",
            "19                                 Other equity      69,401      60,105   \n",
            "\n",
            "   Amount 2022 Amount 2023 Amount 2019  \n",
            "0          202         202          30  \n",
            "1            1           1          31  \n",
            "2            3           3           1  \n",
            "3          906           4       6,923  \n",
            "4            2           2         641  \n",
            "5           67           2       1,247  \n",
            "6            4           5           1  \n",
            "7            5           6         101  \n",
            "8            6           7         049  \n",
            "9          955          17          30  \n",
            "10       5,287          17          30  \n",
            "11           9          10      38,920  \n",
            "12           4           5          13  \n",
            "13           7           8          13  \n",
            "14           8           9      29,416  \n",
            "15           5           6       9,504  \n",
            "16           6           7       2,665  \n",
            "17           9          10       2,455  \n",
            "18          11          12       7,049  \n",
            "19      69,401      67,203       7,825  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all descriptions are strings and handle None values\n",
        "descriptions = [str(desc) if desc is not None else \"\" for desc in descriptions]\n",
        "\n",
        "# Tokenize the descriptions again\n",
        "try:\n",
        "    encodings = tokenizer(descriptions, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "    input_ids = encodings['input_ids'].to(device)\n",
        "    attention_mask = encodings['attention_mask'].to(device)\n",
        "except Exception as e:\n",
        "    print(\"An error occurred during tokenization:\", e)\n"
      ],
      "metadata": {
        "id": "vQAeaIzngeEn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Tokenize Text Descriptions\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Check if GPU is available and set it as the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Tokenize the descriptions\n",
        "encodings = tokenizer(descriptions, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# Convert encodings to tensors and move them to the device\n",
        "input_ids = encodings['input_ids'].to(device)\n",
        "attention_mask = encodings['attention_mask'].to(device)\n"
      ],
      "metadata": {
        "id": "uWJ_oPoFgVjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "034bad89-a663-4e78-bdde-ff67db0f87af"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Dataset and DataLoader\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "# Ensure all amounts are converted to floats, handling non-numeric values\n",
        "amounts_2019_tensor = torch.tensor(response_df['Amount 2019'].apply(pd.to_numeric, errors='coerce').fillna(0).values, dtype=torch.float).to(device)\n",
        "amounts_2020_tensor = torch.tensor(response_df['Amount 2020'].apply(pd.to_numeric, errors='coerce').fillna(0).values, dtype=torch.float).to(device)\n",
        "amounts_2021_tensor = torch.tensor(response_df['Amount 2021'].apply(pd.to_numeric, errors='coerce').fillna(0).values, dtype=torch.float).to(device)\n",
        "amounts_2022_tensor = torch.tensor(response_df['Amount 2022'].apply(pd.to_numeric, errors='coerce').fillna(0).values, dtype=torch.float).to(device)\n",
        "amounts_2023_tensor = torch.tensor(response_df['Amount 2023'].apply(pd.to_numeric, errors='coerce').fillna(0).values, dtype=torch.float).to(device)"
      ],
      "metadata": {
        "id": "y8ZQFFRcyEqM"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the TensorDataset including all years\n",
        "dataset = TensorDataset(input_ids, attention_mask, amounts_2019_tensor, amounts_2020_tensor, amounts_2021_tensor, amounts_2022_tensor, amounts_2023_tensor)\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
      ],
      "metadata": {
        "id": "wkg-rlE2hSFM"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def custom_loss_function(outputs, *targets):\n",
        "    mse_loss = nn.MSELoss()\n",
        "    losses = []\n",
        "    # Check if outputs dimension matches the number of targets\n",
        "    if outputs.shape[1] != len(targets):\n",
        "        raise ValueError(f\"Output dimensions {outputs.shape[1]} do not match the number of targets {len(targets)}\")\n",
        "\n",
        "    for i, target in enumerate(targets):\n",
        "        loss = mse_loss(outputs[:, i], target)\n",
        "        losses.append(loss)\n",
        "\n",
        "    total_loss = sum(losses) / len(losses)\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "RvAP03_Ah46h"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, BertForSequenceClassification\n",
        "\n",
        "# Load the configuration from the pre-trained BERT model\n",
        "config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Adjust the number of labels to match the number of output years (2019-2023)\n",
        "config.num_labels = 5  # This should match the number of years you are predicting\n",
        "\n",
        "# Initialize the model with the custom configuration\n",
        "model = BertForSequenceClassification(config)\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuzXyY-iYalC",
        "outputId": "a2aefb33-8180-4d25-fa40-9d0a4a92ca80"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define the Training Loop\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(4):  # Number of epochs\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_attention_mask, *b_amounts = batch  # Unpack all amounts\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(b_input_ids, attention_mask=b_attention_mask).logits\n",
        "        loss = custom_loss_function(outputs, *b_amounts)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1} | Average Loss: {total_loss / len(dataloader)}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79J_Lh5CJlJF",
        "outputId": "0a84b867-3fb5-43f8-95d4-b2c5376960cb"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Average Loss: 16086.642919218386\n",
            "Epoch 2 | Average Loss: 15751.228641658634\n",
            "Epoch 3 | Average Loss: 15538.279510696213\n",
            "Epoch 4 | Average Loss: 15482.218416387384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model"
      ],
      "metadata": {
        "id": "UHwwYDcz-Bdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming response_df is your full dataset containing all descriptions and amounts\n",
        "train_df, test_df = train_test_split(response_df, test_size=0.2, random_state=42)  # Adjust test_size as needed\n",
        "\n",
        "# Separate descriptions for tokenizing\n",
        "train_texts = train_df['Description'].tolist()\n",
        "test_texts = test_df['Description'].tolist()\n"
      ],
      "metadata": {
        "id": "FOcocIOH741n"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all descriptions are strings and handle None values\n",
        "test_texts = [str(text) if text is not None else \"\" for text in test_texts]\n",
        "\n",
        "# Retry tokenizing the test data\n",
        "try:\n",
        "    test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "    test_input_ids = test_encodings['input_ids'].to(device)\n",
        "    test_attention_mask = test_encodings['attention_mask'].to(device)\n",
        "except Exception as e:\n",
        "    print(\"Failed during tokenization:\", e)\n"
      ],
      "metadata": {
        "id": "PjTBbcuw8H01"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert columns to numeric, coercing errors and filling NaNs\n",
        "test_df['Amount 2019'] = pd.to_numeric(test_df['Amount 2019'], errors='coerce').fillna(0)\n",
        "test_df['Amount 2020'] = pd.to_numeric(test_df['Amount 2020'], errors='coerce').fillna(0)\n",
        "test_df['Amount 2021'] = pd.to_numeric(test_df['Amount 2021'], errors='coerce').fillna(0)\n",
        "test_df['Amount 2022'] = pd.to_numeric(test_df['Amount 2022'], errors='coerce').fillna(0)\n",
        "test_df['Amount 2023'] = pd.to_numeric(test_df['Amount 2023'], errors='coerce').fillna(0)\n",
        "\n",
        "# Convert test amounts to tensors\n",
        "try:\n",
        "    test_amounts_2019 = torch.tensor(test_df['Amount 2019'].values, dtype=torch.float).to(device)\n",
        "    test_amounts_2020 = torch.tensor(test_df['Amount 2020'].values, dtype=torch.float).to(device)\n",
        "    test_amounts_2021 = torch.tensor(test_df['Amount 2021'].values, dtype=torch.float).to(device)\n",
        "    test_amounts_2022 = torch.tensor(test_df['Amount 2022'].values, dtype=torch.float).to(device)\n",
        "    test_amounts_2023 = torch.tensor(test_df['Amount 2023'].values, dtype=torch.float).to(device)\n",
        "except TypeError as e:\n",
        "    print(\"Failed to convert amounts to tensors:\", e)\n"
      ],
      "metadata": {
        "id": "5qsqWAKH74fE"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df[['Amount 2019', 'Amount 2020', 'Amount 2021', 'Amount 2022', 'Amount 2023']].dtypes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ7dd_Qk74ac",
        "outputId": "5e01b1c4-dd3e-4289-872c-55a815285b5e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount 2019    float64\n",
            "Amount 2020    float64\n",
            "Amount 2021    float64\n",
            "Amount 2022    float64\n",
            "Amount 2023    float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the TensorDataset for the test set\n",
        "test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_amounts_2019, test_amounts_2020, test_amounts_2021, test_amounts_2022, test_amounts_2023)\n",
        "\n",
        "# Create DataLoader for the test set\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
      ],
      "metadata": {
        "id": "w0WHyGQs74XM"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Assuming the model and custom_loss_function are already defined\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Initialize test DataLoader\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)  # Make sure test_dataset is defined as shown previously\n",
        "\n",
        "# Disable gradient calculation for evaluation\n",
        "total_eval_loss = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        # Move batch to the appropriate device\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_attention_mask, b_amounts_2019, b_amounts_2020, b_amounts_2021, b_amounts_2022, b_amounts_2023 = batch\n",
        "\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(b_input_ids, attention_mask=b_attention_mask).logits\n",
        "        loss = custom_loss_function(outputs, b_amounts_2019, b_amounts_2020, b_amounts_2021, b_amounts_2022, b_amounts_2023)\n",
        "\n",
        "        # Update test loss\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "# Calculate the average loss over all of the batches\n",
        "average_eval_loss = total_eval_loss / len(test_loader)\n",
        "print(f\"Average Evaluation Loss: {average_eval_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzhE8I3fvZ61",
        "outputId": "da49ec18-97a7-4c84-efa6-97e5d5e67118"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Evaluation Loss: 16083.3772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzJOhhL90g9V",
        "outputId": "a0682e2a-b57d-49fe-b0cf-c0db9566d7da"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Saving the Model\n",
        "model_path = '/content/drive/My Drive/My Models/bert_model_infosys+TCS'\n",
        "tokenizer_path = '/content/drive/My Drive/My Models/bert_model_infosys+TCS'\n",
        "\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(tokenizer_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaArrzGM0q4W",
        "outputId": "501cdba0-340f-4a16-add3-fea006641bd8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/My Models/bert_model_infosys+TCS/tokenizer_config.json',\n",
              " '/content/drive/My Drive/My Models/bert_model_infosys+TCS/special_tokens_map.json',\n",
              " '/content/drive/My Drive/My Models/bert_model_infosys+TCS/vocab.txt',\n",
              " '/content/drive/My Drive/My Models/bert_model_infosys+TCS/added_tokens.json',\n",
              " '/content/drive/My Drive/My Models/bert_model_infosys+TCS/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Ensure the directory exists\n",
        "model_dir = './model'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# save the PyTorch model state dictionary\n",
        "model_save_path = os.path.join(model_dir, 'pytorch_model.bin')\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "# Check if the file is saved correctly\n",
        "!ls -l ./model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNPnuuLSS91I",
        "outputId": "7894a860-a327-4642-a71a-b9fa2f7b5d33"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 427764\n",
            "-rw-r--r-- 1 root root 438024366 Sep 13 05:13 pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Ensure all necessary files are saved in the ./model directory\n",
        "tokenizer.save_pretrained('./model')\n",
        "model.config.save_pretrained('./model')\n",
        "\n",
        "# Copy files from the local Colab environment to Google Drive, overwriting if necessary\n",
        "model_dir = './model'\n",
        "drive_path = '/content/drive/My Drive/My Models/bert_model_infosys+TCS'\n",
        "shutil.copytree(model_dir, drive_path, dirs_exist_ok=True)\n",
        "\n",
        "# Download the files to local machine\n",
        "from google.colab import files\n",
        "files.download('./model/config.json')\n",
        "files.download('./model/pytorch_model.bin')\n",
        "files.download('./model/tokenizer_config.json')\n",
        "files.download('./model/vocab.txt')\n"
      ],
      "metadata": {
        "id": "3Qyhe6Os478m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chatbot"
      ],
      "metadata": {
        "id": "pVFzPNJV8tyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model and tokenizer\n",
        "model_path = '/content/drive/My Drive/My Models/bert_model_infosys+TCS'\n",
        "tokenizer_path = '/content/drive/My Drive/My Models/bert_model_infosys+TCS'\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGOK3unm8zxp",
        "outputId": "21e7055d-2e69-41c6-ded2-8b6c557ac7ca"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Prediction function with extended years\n",
        "def predict(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Apply sigmoid function to get probabilities and then round them to get predicted classes\n",
        "    probabilities = torch.sigmoid(logits)\n",
        "    predicted_classes = torch.round(probabilities).squeeze().tolist()\n",
        "\n",
        "    # Matching the question to the filtered DataFrame\n",
        "    matched_row = filtered_df[filtered_df['Description'].str.lower().apply(lambda x: any(keyword in x for keyword in text.lower().split()))].iloc[0] if not filtered_df[filtered_df['Description'].str.lower().apply(lambda x: any(keyword in x for keyword in text.lower().split()))].empty else None\n",
        "\n",
        "    if matched_row is not None:\n",
        "        amounts = {year: matched_row[f'Amount_{year}'] for year in range(2019, 2024)}\n",
        "    else:\n",
        "        amounts = {year: \"N/A\" for year in range(2019, 2024)}\n",
        "\n",
        "    return predicted_classes, amounts\n"
      ],
      "metadata": {
        "id": "PD-togqP0tGL"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    # Normalize the input text to lowercase for better matching\n",
        "    normalized_text = text.lower()\n",
        "\n",
        "    # Determine the company and year from the user input\n",
        "    company = 'infosys' if 'infosys' in normalized_text else 'tcs'\n",
        "    year = next((y for y in range(2019, 2024) if str(y) in normalized_text), None)\n",
        "\n",
        "    if not year:\n",
        "        return None, \"Please specify a valid year in your query.\"\n",
        "\n",
        "    # Attempt to find a matching row based on the company and year\n",
        "    matched_row = filtered_df[filtered_df['Description'].str.contains(company, case=False, na=False)]\n",
        "    if matched_row.empty:\n",
        "        return None, f\"No data found for {company}.\"\n",
        "\n",
        "    amount = matched_row[f'Amount_{year}'].iloc[0] if not matched_row.empty else \"N/A\"\n",
        "\n",
        "    return amount, f\"The {year} amount for {company} is {amount}.\"\n",
        "\n",
        "# Adjust your chatbot function to use this new predict method\n",
        "def chatbot():\n",
        "    print(\"Bot: Hello! I am your financial assistant. Ask me about the financial data. (Type 'exit' to quit)\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
        "            print(\"Bot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        amount, response = predict(user_input)\n",
        "        print(f\"Bot: {response}\")\n",
        "\n",
        "# Start the chatbot\n",
        "chatbot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy9n-3Vy0tDd",
        "outputId": "e165c3ea-2909-4dde-d2e9-7a4d83540aa6"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: Hello! I am your financial assistant. Ask me about the financial data. (Type 'exit' to quit)\n",
            "You: What was the total asset value for TCS in 2022?\n",
            "Bot: The 2022 amount for tcs is 6,861.\n",
            "You: How have the assets of TCS changed from 2019 to 2023?\n",
            "Bot: The 2019 amount for tcs is 0.\n",
            "You: What was the investment income for TCS in 2022?\n",
            "Bot: The 2022 amount for tcs is 6,861.\n",
            "You: exit\n",
            "Bot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2HBo1L40s-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rAg142Pb0s8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P-q_vbY50s5O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}